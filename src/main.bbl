\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[DeepSeek-AI(2025)]{deepep}
DeepSeek-AI.
\newblock Deepseek-v3 technical report, 2025.
\newblock URL \url{https://arxiv.org/abs/2412.19437}.

\bibitem[AI(2025)]{llama4}
Meta AI.
\newblock The llama 4 herd: The beginning of a new era of natively multimodal
  ai innovation, 2025.
\newblock URL \url{https://ai.meta.com/blog/llama-4-multimodal-intelligence/}.

\bibitem[Research(2024{\natexlab{a}})]{dbrx}
Mosaic Research.
\newblock Introducing dbrx: A new state-of-the-art open llm,
  2024{\natexlab{a}}.
\newblock URL
  \url{https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm}.

\bibitem[Research(2024{\natexlab{b}})]{arctic}
Snowflake~AI Research.
\newblock Snowflake arctic: The best llm for enterprise ai — efficiently
  intelligent, truly open, 2024{\natexlab{b}}.
\newblock URL
  \url{https://www.snowflake.com/en/blog/arctic-open-efficient-foundation-language-models-snowflake/}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{NIPS2017_3f5ee243}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Singh et~al.(2023)Singh, Ruwase, Awan, Rajbhandari, He, and
  Bhatele]{10.1145/3577193.3593704}
Siddharth Singh, Olatunji Ruwase, Ammar~Ahmad Awan, Samyam Rajbhandari, Yuxiong
  He, and Abhinav Bhatele.
\newblock A hybrid tensor-expert-data parallelism approach to optimize
  mixture-of-experts training.
\newblock In \emph{Proceedings of the 37th ACM International Conference on
  Supercomputing}, ICS '23, page 203–214, New York, NY, USA, 2023.
  Association for Computing Machinery.
\newblock ISBN 9798400700569.
\newblock \doi{10.1145/3577193.3593704}.
\newblock URL \url{https://doi.org/10.1145/3577193.3593704}.

\bibitem[Liu et~al.(2023)Liu, Wang, and Jiang]{10.1145/3603269.3604869}
Juncai Liu, Jessie~Hui Wang, and Yimin Jiang.
\newblock Janus: A unified distributed training framework for sparse
  mixture-of-experts models.
\newblock In \emph{Proceedings of the ACM SIGCOMM 2023 Conference}, ACM SIGCOMM
  '23, page 486–498, New York, NY, USA, 2023. Association for Computing
  Machinery.
\newblock ISBN 9798400702365.
\newblock \doi{10.1145/3603269.3604869}.
\newblock URL \url{https://doi.org/10.1145/3603269.3604869}.

\bibitem[Jiang et~al.(2024{\natexlab{a}})Jiang, Tian, Jia, Zheng, Wu, and
  Wang]{MLSYS2024_339caf45}
Chenyu Jiang, Ye~Tian, Zhen Jia, Shuai Zheng, Chuan Wu, and Yida Wang.
\newblock Lancet: Accelerating mixture-of-experts training via whole graph
  computation-communication overlapping.
\newblock In P.~Gibbons, G.~Pekhimenko, and C.~De Sa, editors,
  \emph{Proceedings of Machine Learning and Systems}, volume~6, pages 74--86,
  2024{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.mlsys.org/paper_files/paper/2024/file/339caf45a6fa281cae8adc6465343464-Paper-Conference.pdf}.

\bibitem[ncc()]{nccl}
{NVIDIA Collective Communications Library (NCCL)}.
\newblock \url{https://developer.nvidia.com/nccl}.

\bibitem[Wendt and Wyatt(2019)]{cuda_graphs_nvidia_blog}
Michael Wendt and Joshua Wyatt.
\newblock Getting started with {CUDA} graphs.
\newblock \url{https://developer.nvidia.com/blog/cuda-graphs/}, 2019.
\newblock Accessed: 2024-05-15.

\bibitem[NVIDIA(2025{\natexlab{a}})]{nvshm}
NVIDIA.
\newblock Nvidia openshmem library (nvshmem), 2025{\natexlab{a}}.
\newblock URL \url{https://docs.nvidia.com/nvshmem/api/index.html}.
\newblock v3.2.5.

\bibitem[Thakkar et~al.(2025)Thakkar, Ramani, Cecka, Shivam, Lu, Yan, Kosaian,
  Hoemmen, Wu, Kerr, Nicely, Merrill, Blasig, Qiao, Majcher, Springer,
  Hohnerbach, Wang, and Gupta]{Thakkar_CUTLASS_2023}
Vijay Thakkar, Pradeep Ramani, Cris Cecka, Aniket Shivam, Honghao Lu, Ethan
  Yan, Jack Kosaian, Mark Hoemmen, Haicheng Wu, Andrew Kerr, Matt Nicely, Duane
  Merrill, Dustyn Blasig, Fengqi Qiao, Piotr Majcher, Paul Springer, Markus
  Hohnerbach, Jin Wang, and Manish Gupta.
\newblock {CUTLASS}, 2025.
\newblock URL \url{https://github.com/NVIDIA/cutlass}.

\bibitem[Zhang et~al.()Zhang, Zheng, Lin, Jiang, Bao, Jiang, Hou, Cui, Zheng,
  Chang, Chen, and Liu]{comet}
Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan
  Jiang, Qi~Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, and Xin Liu.
\newblock Comet: Fine-grained computation-communication overlapping for
  mixture-of-experts.
\newblock In \emph{MLSys '25}.
\newblock URL \url{https://arxiv.org/abs/2502.19811}.

\bibitem[NVIDIA(2025{\natexlab{b}})]{megatron}
NVIDIA.
\newblock Megatron-lm, 2025{\natexlab{b}}.
\newblock URL \url{https://github.com/NVIDIA/Megatron-LM?tab=readme-ov-file}.
\newblock v0.11.0.

\bibitem[Narayanan et~al.(2021{\natexlab{a}})Narayanan, Shoeybi, Casper,
  LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro,
  Phanishayee, and Zaharia]{10.1145/3458817.3476209}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, Amar Phanishayee, and Matei Zaharia.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, SC '21, New York,
  NY, USA, 2021{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9781450384421.
\newblock \doi{10.1145/3458817.3476209}.
\newblock URL \url{https://doi.org/10.1145/3458817.3476209}.

\bibitem[Rajbhandari et~al.(2022)Rajbhandari, Li, Yao, Zhang, Aminabadi, Awan,
  Rasley, and He]{pmlr-v162-rajbhandari22a}
Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza~Yazdani
  Aminabadi, Ammar~Ahmad Awan, Jeff Rasley, and Yuxiong He.
\newblock {D}eep{S}peed-{M}o{E}: Advancing mixture-of-experts inference and
  training to power next-generation {AI} scale.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, volume 162 of \emph{Proceedings of Machine Learning Research},
  pages 18332--18346. PMLR, 17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/rajbhandari22a.html}.

\bibitem[NERSC(2025)]{nerscNetworkNERSC}
NERSC.
\newblock {N}etwork - {N}{E}{R}{S}{C} {D}ocumentation.
\newblock \url{https://docs.nersc.gov/performance/network/}, 2025.
\newblock [Accessed 23-05-2025].

\bibitem[Bell et~al.(2006)Bell, Bonachea, Nishtala, and Yelick]{1639320}
C.~Bell, D.~Bonachea, R.~Nishtala, and K.~Yelick.
\newblock Optimizing bandwidth limited problems using one-sided communication
  and overlap.
\newblock In \emph{Proceedings 20th IEEE International Parallel \& Distributed
  Processing Symposium}, pages 10 pp.--, 2006.
\newblock \doi{10.1109/IPDPS.2006.1639320}.

\bibitem[Chen et~al.(2023)Chen, Brock, Porumbescu, Buluc, Yelick, and
  Owens]{10.1145/3545008.3545056}
Yuxin Chen, Benjamin Brock, Serban Porumbescu, Aydin Buluc, Katherine Yelick,
  and John Owens.
\newblock Atos: A task-parallel gpu scheduler for graph analytics.
\newblock In \emph{Proceedings of the 51st International Conference on Parallel
  Processing}, ICPP '22, New York, NY, USA, 2023. Association for Computing
  Machinery.
\newblock ISBN 9781450397339.
\newblock \doi{10.1145/3545008.3545056}.
\newblock URL \url{https://doi.org/10.1145/3545008.3545056}.

\bibitem[Hwang et~al.(2023{\natexlab{a}})Hwang, Cui, Xiong, Yang, Liu, Hu,
  Wang, Salas, Jose, Ram, Chau, Cheng, Yang, Yang, and
  Xiong]{MLSYS2023_5616d34c}
Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze~Liu, Han Hu, Zilong Wang,
  Rafael Salas, Jithin Jose, Prabhat Ram, HoYuen Chau, Peng Cheng, Fan Yang,
  Mao Yang, and Yongqiang Xiong.
\newblock Tutel: Adaptive mixture-of-experts at scale.
\newblock In D.~Song, M.~Carbin, and T.~Chen, editors, \emph{Proceedings of
  Machine Learning and Systems}, volume~5, pages 269--287. Curan,
  2023{\natexlab{a}}.
\newblock URL
  \url{https://proceedings.mlsys.org/paper_files/paper/2023/file/5616d34cf8ff73942cfd5aa922842556-Paper-mlsys2023.pdf}.

\bibitem[He et~al.(2022{\natexlab{a}})He, Zhai, Antunes, Wang, Luo, Shi, and
  Li]{10.1145/3503221.3508418}
Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi,
  and Qin Li.
\newblock Fastermoe: modeling and optimizing training of large-scale dynamic
  pre-trained models.
\newblock In \emph{Proceedings of the 27th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, PPoPP '22, page 120–134, New York,
  NY, USA, 2022{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9781450392044.
\newblock \doi{10.1145/3503221.3508418}.
\newblock URL \url{https://doi.org/10.1145/3503221.3508418}.

\bibitem[Nie et~al.(2023)Nie, Miao, Wang, Yang, Xue, Ma, Cao, and
  Cui]{10.1145/3588964}
Xiaonan Nie, Xupeng Miao, Zilong Wang, Zichao Yang, Jilong Xue, Lingxiao Ma,
  Gang Cao, and Bin Cui.
\newblock Flexmoe: Scaling large-scale sparse pre-trained model training via
  dynamic device placement.
\newblock \emph{Proc. ACM Manag. Data}, 1\penalty0 (1), May 2023.
\newblock \doi{10.1145/3588964}.
\newblock URL \url{https://doi.org/10.1145/3588964}.

\bibitem[Shi et~al.(2024)Shi, Pan, Wang, Liu, Ren, Hu, Yang, Li, and
  Chu]{10.1145/3627703.3650083}
Shaohuai Shi, Xinglin Pan, Qiang Wang, Chengjian Liu, Xiaozhe Ren, Zhongzhe Hu,
  Yu~Yang, Bo~Li, and Xiaowen Chu.
\newblock Schemoe: An extensible mixture-of-experts distributed training system
  with tasks scheduling.
\newblock In \emph{Proceedings of the Nineteenth European Conference on
  Computer Systems}, EuroSys '24, page 236–249, New York, NY, USA, 2024.
  Association for Computing Machinery.
\newblock ISBN 9798400704376.
\newblock \doi{10.1145/3627703.3650083}.
\newblock URL \url{https://doi.org/10.1145/3627703.3650083}.

\bibitem[Wang et~al.(2025{\natexlab{a}})Wang, Xia, Yang, Zhou, and
  Cheng]{10.1145/3710848.3710868}
Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng.
\newblock Harnessing inter-gpu shared memory for seamless moe
  communication-computation fusion.
\newblock In \emph{Proceedings of the 30th ACM SIGPLAN Annual Symposium on
  Principles and Practice of Parallel Programming}, PPoPP '25, page 170–182,
  New York, NY, USA, 2025{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9798400714436.
\newblock \doi{10.1145/3710848.3710868}.
\newblock URL \url{https://doi.org/10.1145/3710848.3710868}.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and
  R\'{e}]{NEURIPS2022_67d57c32}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\'{e}.
\newblock Flashattention: Fast and memory-efficient exact attention with
  io-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pages 16344--16359. Curran Associates, Inc., 2022.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/
  2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf}.

\bibitem[Agha(1985)]{agha:85}
Gul~A. Agha.
\newblock Actors: A model of concurrent computation in distributed systems.
\newblock Technical report, 1985.
\newblock MIT Artificial Intelligence Laboratory Technical Reports.

\bibitem[Hewitt et~al.(1973)Hewitt, Bishop, and
  Steiger]{10.5555/1624775.1624804}
Carl Hewitt, Peter Bishop, and Richard Steiger.
\newblock A universal modular actor formalism for artificial intelligence.
\newblock IJCAI'73, page 235–245, San Francisco, CA, USA, 1973. Morgan
  Kaufmann Publishers Inc.

\bibitem[Greif(1975)]{Greif:75}
Irene Greif.
\newblock \emph{SEMANTICS OF COMMUNICATING PARALLEL PROCESSES}.
\newblock PhD thesis, Massachusetts Institute of Technology, 1975.

\bibitem[Luo et~al.(2024)Luo, Fan, Li, Du, Wang, and Chu]{10579250}
Weile Luo, Ruibo Fan, Zeyu Li, Dayou Du, Qiang Wang, and Xiaowen Chu.
\newblock { Benchmarking and Dissecting the Nvidia Hopper GPU Architecture }.
\newblock In \emph{2024 IEEE International Parallel and Distributed Processing
  Symposium (IPDPS)}, pages 656--667, Los Alamitos, CA, USA, May 2024. IEEE
  Computer Society.
\newblock \doi{10.1109/IPDPS57955.2024.00064}.
\newblock URL
  \url{https://doi.ieeecomputersociety.org/10.1109/IPDPS57955.2024.00064}.

\bibitem[Abdelkhalik et~al.(2022)Abdelkhalik, Arafa, Santhi, and
  Badawy]{amperearch}
Hamdy Abdelkhalik, Yehia Arafa, Nandakishore Santhi, and Abdel-Hameed Badawy.
\newblock Demystifying the nvidia ampere architecture through microbenchmarking
  and instruction-level analysis, 2022.
\newblock URL \url{https://arxiv.org/abs/2208.11174}.

\bibitem[NVIDIA(2025{\natexlab{c}})]{ptx}
NVIDIA.
\newblock Ptx isa: Version 8.7, 2025{\natexlab{c}}.
\newblock URL \url{https://docs.nvidia.com/cuda/pdf/ptx_isa_8.7.pdf}.

\bibitem[Lepikhin et~al.(2021)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{DBLP:conf/iclr/LepikhinLXCFHKS21}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock In \emph{9th International Conference on Learning Representations,
  {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=qrwe7XHTmYb}.

\bibitem[Yelick et~al.(2007)Yelick, Bonachea, Chen, Colella, Datta, Duell,
  Graham, Hargrove, Hilfinger, Husbands, Iancu, Kamil, Nishtala, Su, Welcome,
  and Wen]{10.1145/1278177.1278183}
Katherine Yelick, Dan Bonachea, Wei-Yu Chen, Phillip Colella, Kaushik Datta,
  Jason Duell, Susan~L. Graham, Paul Hargrove, Paul Hilfinger, Parry Husbands,
  Costin Iancu, Amir Kamil, Rajesh Nishtala, Jimmy Su, Michael Welcome, and
  Tong Wen.
\newblock Productivity and performance using partitioned global address space
  languages.
\newblock In \emph{Proceedings of the 2007 International Workshop on Parallel
  Symbolic Computation}, PASCO '07, page 24–32, New York, NY, USA, 2007.
  Association for Computing Machinery.
\newblock ISBN 9781595937414.
\newblock \doi{10.1145/1278177.1278183}.
\newblock URL \url{https://doi.org/10.1145/1278177.1278183}.

\bibitem[Zheng et~al.(2025)Zheng, Bao, Hou, Zheng, Fang, Huang, Li, Duanmu,
  Chen, Xu, Guo, Zheng, Jiang, Di, Wang, Ye, Lin, Chang, Lu, Liang, Zhai, and
  Liu]{triton-dist}
Size Zheng, Wenlei Bao, Qi~Hou, Xuegui Zheng, Jin Fang, Chenhui Huang, Tianqi
  Li, Haojie Duanmu, Renze Chen, Ruifan Xu, Yifan Guo, Ningxin Zheng, Ziheng
  Jiang, Xinyi Di, Dongyang Wang, Jianxi Ye, Haibin Lin, Li-Wen Chang, Liqiang
  Lu, Yun Liang, Jidong Zhai, and Xin Liu.
\newblock Triton-distributed: Programming overlapping kernels on distributed ai
  systems with the triton compiler, 2025.
\newblock URL \url{https://arxiv.org/abs/2504.19442}.

\bibitem[Anthony et~al.(2024)Anthony, Tokpanov, Glorioso, and Millidge]{bmamba}
Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidge.
\newblock Blackmamba: Mixture of experts for state-space models, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.01771}.

\bibitem[He et~al.(2022{\natexlab{b}})He, Zhai, Antunes, Wang, Luo, Shi, and
  Li]{fastermoe}
Jiaao He, Jidong Zhai, Tiago Antunes, Haojie Wang, Fuwen Luo, Shangfeng Shi,
  and Qin Li.
\newblock Fastermoe: modeling and optimizing training of large-scale dynamic
  pre-trained models.
\newblock In \emph{Proceedings of the 27th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming (PPoPP 22)}, pages 120--134,
  2022{\natexlab{b}}.

\bibitem[Narayanan et~al.(2021{\natexlab{b}})Narayanan, Shoeybi, Casper,
  LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro,
  et~al.]{megatron-lm}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, et~al.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm.
\newblock In \emph{Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--15,
  2021{\natexlab{b}}.

\bibitem[NVIDIA({\natexlab{a}})]{transformer-engine}
NVIDIA.
\newblock Transformer engine, {\natexlab{a}}.
\newblock URL \url{https://github.com/NVIDIA/TransformerEngine}.

\bibitem[NVIDIA({\natexlab{b}})]{nsight-metrics}
NVIDIA.
\newblock {NVIDIA Nsight Systems Metrics}, {\natexlab{b}}.
\newblock URL
  \url{https://docs.nvidia.com/nsight-systems/UserGuide/index.html?highlight=SM%2520active#available-metrics}.

\bibitem[Jangda et~al.(2022)Jangda, Huang, Liu, Sabet, Maleki, Miao, Musuvathi,
  Mytkowicz, and Saarikivi]{coconet}
Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein~Nodehi Sabet, Saeed
  Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, and Olli Saarikivi.
\newblock Breaking the computation and communication abstraction barrier in
  distributed machine learning workloads.
\newblock In \emph{Proceedings of the 27th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems (ASPLOS
  22)}, pages 402--416, 2022.

\bibitem[Wang et~al.(2022)Wang, Wei, Sabne, Davis, Ilbeyi, Hechtman, Chen,
  Murthy, Maggioni, Zhang, et~al.]{decomposition}
Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake
  Hechtman, Dehao Chen, Karthik~Srinivasa Murthy, Marcello Maggioni, Qiao
  Zhang, et~al.
\newblock Overlap communication with dependent computation via decomposition in
  large deep learning models.
\newblock In \emph{Proceedings of the 28th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems (ASPLOS
  22)}, pages 93--106, 2022.

\bibitem[Chen et~al.(2024)Chen, Li, Zhu, Duan, Sun, Zhang, and Yang]{centauri}
Chang Chen, Xiuhong Li, Qianchao Zhu, Jiangfei Duan, Peng Sun, Xingcheng Zhang,
  and Chao Yang.
\newblock Centauri: Enabling efficient scheduling for communication-computation
  overlap in large model training via communication partitioning.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems (ASPLOS
  24)}, pages 178--191, 2024.

\bibitem[Pati et~al.(2024)Pati, Aga, Islam, Jayasena, and Sinclair]{t3}
Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena, and Matthew~D
  Sinclair.
\newblock T3: Transparent tracking \& triggering for fine-grained overlap of
  compute \& collectives.
\newblock In \emph{Proceedings of the 29th ACM International Conference on
  Architectural Support for Programming Languages and Operating Systems (ASPLOS
  24)}, pages 1146--1164, 2024.

\bibitem[Jiang et~al.(2024{\natexlab{b}})Jiang, Lin, Zhong, Huang, Chen, Zhang,
  Peng, Li, Xie, Nong, et~al.]{megascale}
Ziheng Jiang, Haibin Lin, Yinmin Zhong, Qi~Huang, Yangrui Chen, Zhi Zhang,
  Yanghua Peng, Xiang Li, Cong Xie, Shibiao Nong, et~al.
\newblock $\{$MegaScale$\}$: Scaling large language model training to more than
  10,000 $\{$GPUs$\}$.
\newblock In \emph{21st USENIX Symposium on Networked Systems Design and
  Implementation (NSDI 24)}, pages 745--760, 2024{\natexlab{b}}.

\bibitem[Sun et~al.(2024)Sun, Qin, Sun, Li, Li, Shen, Qiao, and Zhong]{co2}
Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu~Qiao, and
  Yiran Zhong.
\newblock {CO}2: Efficient distributed training with full
  communication-computation overlap.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations (ICLR 24)}, 2024.

\bibitem[Mahajan et~al.(2023)Mahajan, Chu, Sridharan, and Akella]{syndicate}
Kshiteej Mahajan, Ching-Hsiang Chu, Srinivas Sridharan, and Aditya Akella.
\newblock Better together: Jointly optimizing $\{$ML$\}$ collective scheduling
  and execution planning using $\{$SYNDICATE$\}$.
\newblock In \emph{20th USENIX Symposium on Networked Systems Design and
  Implementation (NSDI 23)}, pages 809--824, 2023.

\bibitem[Wang et~al.(2025{\natexlab{b}})Wang, Xia, Yang, Zhou, and
  Cheng]{ccfuser}
Hulin Wang, Yaqi Xia, Donglin Yang, Xiaobo Zhou, and Dazhao Cheng.
\newblock Harnessing inter-gpu shared memory for seamless moe
  communication-computation fusion.
\newblock In \emph{Proceedings of the 30th ACM SIGPLAN Annual Symposium on
  Principles and Practice of Parallel Programming}, pages 170--182,
  2025{\natexlab{b}}.

\bibitem[Punniyamurthy et~al.(2024)Punniyamurthy, Hamidouche, and
  Beckmann]{fused}
Kishore Punniyamurthy, Khaled Hamidouche, and Bradford~M Beckmann.
\newblock Optimizing distributed ml communication with fused
  computation-collective operations.
\newblock In \emph{SC24: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pages 1--17, 2024.

\bibitem[Hwang et~al.(2023{\natexlab{b}})Hwang, Cui, Xiong, Yang, Liu, Hu,
  Wang, Salas, Jose, Ram, et~al.]{tutel}
Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze~Liu, Han Hu, Zilong Wang,
  Rafael Salas, Jithin Jose, Prabhat Ram, et~al.
\newblock Tutel: Adaptive mixture-of-experts at scale.
\newblock \emph{Proceedings of Machine Learning and Systems (MLSys 23)},
  5:\penalty0 269--287, 2023{\natexlab{b}}.

\bibitem[Jiang et~al.(2024{\natexlab{c}})Jiang, Tian, Jia, Zheng, Wu, and
  Wang]{lancet}
Chenyu Jiang, Ye~Tian, Zhen Jia, Shuai Zheng, Chuan Wu, and Yida Wang.
\newblock Lancet: Accelerating mixture-of-experts training via whole graph
  computation-communication overlapping.
\newblock In \emph{Proceedings of Machine Learning and Systems (MLSys 24)},
  pages 74--86, 2024{\natexlab{c}}.

\bibitem[Group(2025)]{ofiwgFi_cxi7}
OpenFabrics Interfaces~Working Group.
\newblock fi\_cxi.
\newblock \url{https://ofiwg.github.io/libfabric/v1.21.0/man/fi_cxi.7.html},
  2025.
\newblock [Accessed 23-05-2025].

\end{thebibliography}
