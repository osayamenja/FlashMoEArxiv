\section{Related Work}
\label{sec:related}
\noindent\textbf{Computation-Communication Overlap and Kernel Fusion.}
To reduce the communication overheads of synchronization in distributed DNN training, many research efforts have been focused on
increasing the overlap of computation and communication.
For generic Transformer-based models without MoE layers,
many works~\cite{coconet,decomposition,centauri,t3,megascale,co2,syndicate,ccfuser,fused}
have provided insights and techniques to
partition and schedule computation and communication operations, aimed at
finer-grained overlapping.
To address the challenges posed by \alltoall communication and
expert parallelism in MoE training, Tutel~\cite{tutel} and FasterMoE~\cite{fastermoe}
overlap \alltoall with expert computation.
Lancet~\cite{lancet} additionally enables both non-MoE computation in
forward pass and weight gradient computation in backward pass to be overlapped with \alltoall.
Despite overlapping, the performance of these approaches is
limited in practice due to blocking synchronous collective communication with barriers.
In contrast, \sysname fundamentally
eliminates these inefficiencies with
asynchronous, device-initiated data transfers overlapped with tiled computation
all \emph{within a single kernel}.
\sysname further differentiates itself from SOTA works like COMET~\cite{comet} and DeepEP~\cite{deepep},
which also use this form of kernel-initiated communication but at a coarse-grained granularity
and without complete kernel fusion.

