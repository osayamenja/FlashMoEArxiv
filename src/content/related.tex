\section{Related Work}
\label{sec:related}
\noindent\textbf{Computation-Communication Overlap.}
To reduce the communication overheads of synchronization in distributed DNN training, many research efforts have been focused on
increasing the overlap of computation and communication.
For generic Transformer-based models without MoE layers,
many works~\cite{coconet,decomposition,centauri,t3,megascale,co2,syndicate,ccfuser,fused}
have provided insights and techniques to
partition and schedule computation and communication operations, a
imed at finer-grained overlapping.
To address the challenges posed by \alltoall communication and
expert parallelism in MoE training, Tutel~\cite{tutel} and FasterMoE~\cite{fastermoe}
overlap \alltoall with expert computation.
Lancet~\cite{lancet} additionally enables both non-MoE computation in
forward pass and weight gradient computation in backward pass to be overlapped with \alltoall.
Despite overlapping, the performance of these approaches is
limited in practice due to blocking synchronous collective communication with barriers.
In contrast, \sysname fundamentally
eliminates the inefficiencies with
asynchronous, device-initiated data transfers overlapped with tiled computation
all \emph{within a single kernel},
further differentiating itself from SOTA works~\cite{triton-dist, comet, deepep}
who also use this form of kernel-initiated communication but at a coarse-grained granularity
and without complete kernel fusion.

